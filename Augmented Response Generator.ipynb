{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jgqUSZHcJcjU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zD2O-hiWJ1BD"
      },
      "outputs": [],
      "source": [
        "# Load your dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/SA Capstone/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qu5xfz7qKiNN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create a function to preprocess the dataset for T5\n",
        "def preprocess_data(data):\n",
        "    inputs = \"instruction: \" + data[\"instruction\"]\n",
        "    targets = data[\"response\"]\n",
        "    return inputs, targets\n",
        "\n",
        "train_inputs, train_targets = preprocess_data(train_data)\n",
        "test_inputs, test_targets = preprocess_data(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRrinQg3KlEh",
        "outputId": "6129c1a0-9489-444e-9428-91e150df42c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load the pre-trained T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWTtmum8Knkp"
      },
      "outputs": [],
      "source": [
        "# Tokenize the data\n",
        "train_encodings = tokenizer(train_inputs.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "train_labels = tokenizer(train_targets.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(test_inputs.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_labels = tokenizer(test_targets.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqPCpMZ-ONAm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[\"input_ids\"][idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels[\"input_ids\"])\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "test_dataset = CustomDataset(test_encodings, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "ghDekpO8OSdI",
        "outputId": "07a3b973-5447-4866-972e-e4aa9bc90397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-6-dbef36069641>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-6-dbef36069641>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item[\"labels\"] = torch.tensor(self.labels[\"input_ids\"][idx])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9069' max='9069' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9069/9069 1:06:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.292100</td>\n",
              "      <td>0.247166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.253400</td>\n",
              "      <td>0.223426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.240900</td>\n",
              "      <td>0.215609</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-dbef36069641>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-6-dbef36069641>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item[\"labels\"] = torch.tensor(self.labels[\"input_ids\"][idx])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9069, training_loss=0.2869518974910748, metrics={'train_runtime': 3981.649, 'train_samples_per_second': 18.222, 'train_steps_per_second': 2.278, 'total_flos': 709599179833344.0, 'train_loss': 0.2869518974910748, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "# !pip install transformers[torch] accelerate -U\n",
        "# pip install transformers[torch]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.02,\n",
        "    save_total_limit=1,\n",
        "    save_steps=10_000,\n",
        ")\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOEfKZDQOU7o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "fbc00a3e-746b-4867-b12c-6377d25c82d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-dbef36069641>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-6-dbef36069641>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item[\"labels\"] = torch.tensor(self.labels[\"input_ids\"][idx])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [336/336 00:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.21560907363891602, 'eval_runtime': 43.5546, 'eval_samples_per_second': 61.716, 'eval_steps_per_second': 7.714, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a7Nh3q_5OXNo"
      },
      "outputs": [],
      "source": [
        "# def generate_response(instruction):\n",
        "#     inputs = tokenizer(\"instruction: \" + instruction, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "#     outputs = model.generate(inputs[\"input_ids\"])\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def generate_response(instruction):\n",
        "    inputs = tokenizer(\"instruction: \" + instruction, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Add this line\n",
        "    outputs = model.generate(inputs[\"input_ids\"],\n",
        "                             max_length=256,\n",
        "                              min_length=10,\n",
        "                              num_beams=5,\n",
        "                              early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instruction = \"Create my profile\"\n",
        "# response = generate_response(instruction)\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv9ztIHiqhwY",
        "outputId": "9248819a-19eb-4887-98c5-d62a16abaaaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for expressing your interest in creating a profile! I'm here to assist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "\n",
        "    print(\"Chatbot: Hi! I am your virtual assistance,Feel free to ask, and I'll do my best to provide you with answers and assistance..\")\n",
        "    print(\"Type 'quit' to exit the chat\\n\\n\")\n",
        "\n",
        "    text = input(\"User: \")\n",
        "\n",
        "    while(text != 'quit'):\n",
        "\n",
        "\n",
        "        response = generate_response(text)\n",
        "\n",
        "        print(f\"Chatbot: {response}\\n\\n\")\n",
        "\n",
        "        text = input(\"User: \")"
      ],
      "metadata": {
        "id": "zrmxSvOwrmyd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKvi1-4gsLWL",
        "outputId": "1f4e4788-6d31-4c4f-f60c-5445e0ff4229"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Hi! I am your virtual assistance,Feel free to ask, and I'll do my best to provide you with answers and assistance..\n",
            "Type 'quit' to exit the chat\n",
            "\n",
            "\n",
            "User: Hi!\n",
            "Chatbot: Thank you for reaching out! I'm here to assist you in resolving this matter. To get started, could you please provide me with more details about the specific issue you're facing? This will help me guide you through the process and ensure a seamless experience.\n",
            "\n",
            "\n",
            "User: I need assistance for creating a new account\n",
            "Chatbot: Thank you for expressing your interest in creating a new account! I'm here to assist you every step of the way. To get started, could you please provide me with your full name, email address, and a preferred username? Once I have these details, I'll guide you through the account creation process, ensuring a smooth and hassle-free experience. Rest assured, your security and privacy are of utmost importance to us, and we will handle your information with the utmost care. Let's get started on this exciting journey together!\n",
            "\n",
            "\n",
            "User: I need to track my package\n",
            "Chatbot: Thank you for reaching out! I'm here to assist you in tracking your package. To track your package, you can visit our website and navigate to the \"Order Tracking\" or \"Track Your Package\" section. There, you should be able to find the most up-to-date information regarding your package. If you encounter any difficulties or have further questions, please don't hesitate to let me know. I'm here to help!\n",
            "\n",
            "\n",
            "User: Delete my account\n",
            "Chatbot: Appreciate you reaching out to us regarding the deletion of your account. I understand that you would like to delete your account, and I'm here to assist you with that. To proceed with the deletion, please provide me with the necessary details such as your account username or email associated with the account. Once I have this information, I will guide you through the process step by step, ensuring a smooth account deletion experience.\n",
            "\n",
            "\n",
            "User: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model.save_pretrained('/content/drive/MyDrive/SA Capstone/model_t5_small')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/SA Capstone/tokenizer_t5_small')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F3egvYst3I0",
        "outputId": "a11aebfa-00fb-4fe1-c30a-dd4c4a5a190f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/SA Capstone/tokenizer_t5_small/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/SA Capstone/tokenizer_t5_small/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/SA Capstone/tokenizer_t5_small/spiece.model',\n",
              " '/content/drive/MyDrive/SA Capstone/tokenizer_t5_small/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/SA Capstone/model_t5_small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('/content/drive/MyDrive/SA Capstone/tokenizer_t5_small')"
      ],
      "metadata": {
        "id": "k5qNaOKJxcVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25608a40-e816-491a-d28e-95c2fbe108c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}